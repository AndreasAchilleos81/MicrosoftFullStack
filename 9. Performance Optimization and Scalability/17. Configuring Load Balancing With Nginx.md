# NGINX Load Balancing Configuration Guide

## Introduction

When you're developing a backend where you expect a lot of users to be getting data or interacting with that backend, it's important to create an environment where you can balance the load on any one server by spreading out that load, the amount of traffic going to multiple servers. Load balancing is important because it helps you distribute how much traffic each server will get.

There's software that can help you with load balancing. We're going to use an open source software package called **NGINX** (spelled N-G-I-N-X). NGINX is developed to help automatically balance the load between multiple servers or multiple applications that are running.

## Setting Up Backend Servers

### Original Node.js Example

The demonstration uses two simple Node.js servers to replicate servers that would be running applications. You can distribute the load between multiple servers either on completely separate machines or on an individual machine.

**server1.js** (listening on port 5001):
```javascript
const http = require('http');

const server = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('Hello from backend1\n');
});

const PORT = 5001;
server.listen(PORT, () => {
  console.log(`Backend1 is running on port ${PORT}`);
});
```

**server2.js** (listening on port 5002):
```javascript
const http = require('http');

const server = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('Hello from backend2\n');
});

const PORT = 5002;
server.listen(PORT, () => {
  console.log(`Backend2 is running on port ${PORT}`);
});
```

### C# ASP.NET Core Implementation

Here's how to create equivalent backend servers using C# and ASP.NET Core:

**Program.cs for Backend Server 1** (port 5001):
```csharp
var builder = WebApplication.CreateBuilder(args);

// Configure Kestrel to listen on port 5001
builder.WebHost.ConfigureKestrel(options =>
{
    options.ListenLocalhost(5001);
});

var app = builder.Build();

app.MapGet("/", () => "Hello from backend1");

Console.WriteLine("Backend1 is running on port 5001");
app.Run();
```

**Program.cs for Backend Server 2** (port 5002):
```csharp
var builder = WebApplication.CreateBuilder(args);

// Configure Kestrel to listen on port 5002
builder.WebHost.ConfigureKestrel(options =>
{
    options.ListenLocalhost(5002);
});

var app = builder.Build();

app.MapGet("/", () => "Hello from backend2");

Console.WriteLine("Backend2 is running on port 5002");
app.Run();
```

### Alternative: Single C# Project with Multiple Configurations

You can also create a single C# project and use configuration files:

**Program.cs**:
```csharp
var builder = WebApplication.CreateBuilder(args);

// Read port from configuration or command line
var port = builder.Configuration.GetValue<int>("Port", 5001);
var serverName = builder.Configuration.GetValue<string>("ServerName", "backend1");

builder.WebHost.ConfigureKestrel(options =>
{
    options.ListenLocalhost(port);
});

var app = builder.Build();

app.MapGet("/", () => $"Hello from {serverName}");

Console.WriteLine($"{serverName} is running on port {port}");
app.Run();
```

Run with command line arguments:
```bash
# Terminal 1
dotnet run --Port=5001 --ServerName=backend1

# Terminal 2
dotnet run --Port=5002 --ServerName=backend2
```

## NGINX Configuration

### Basic Configuration

The NGINX configuration defines upstream backend servers and how to distribute traffic:

**nginx.conf**:
```nginx
http {
    upstream backend_servers {
        server 127.0.0.1:5001;
        server 127.0.0.1:5002;
    }

    server {
        listen 80;
        server_name localhost;

        location / {
            proxy_pass http://backend_servers;
        }
    }
}
```

This configuration tells NGINX:
- You have two servers available on ports 5001 and 5002
- NGINX should listen on port 80 for incoming requests
- Requests to localhost should be distributed across the backend servers

### Configuration with Health Checks

You can also configure NGINX to manage server health:

```nginx
http {
    upstream backend_servers {
        server 127.0.0.1:5001 max_fails=3 fail_timeout=30s;
        server 127.0.0.1:5002 max_fails=3 fail_timeout=30s;
    }

    server {
        listen 80;
        server_name localhost;

        location / {
            proxy_pass http://backend_servers;
            proxy_next_upstream error timeout invalid_header http_500;
        }
    }
}
```

## Running the Demonstration

### Starting the Servers

**For Node.js:**
```bash
# Terminal 1
node server1.js

# Terminal 2
node server2.js
```

**For C#:**
```bash
# Terminal 1
cd BackendServer1
dotnet run

# Terminal 2
cd BackendServer2
dotnet run
```

### Testing Load Distribution

Once both servers are running, you can test the load balancing using curl:

```bash
curl http://localhost
```

**First request output:**
```
Hello from backend1
```

**Second request output:**
```
Hello from backend2
```

**Third request output:**
```
Hello from backend1
```

NGINX distributes the load using a round-robin algorithm by default, alternating between the two servers with each request.

## Health Check Demonstration

### Simulating Server Failure

To illustrate health management, stop server2. If NGINX detects that server2 is no longer available, it will automatically distribute all traffic to the only active server.

**Stop server2** (Ctrl+C in the terminal running server2)

**Run curl command again:**
```bash
curl http://localhost
```

**Every subsequent request will return:**
```
Hello from backend1
```

NGINX automatically detects that server2 is down and routes all traffic to backend1, ensuring continuous service availability.

## Benefits of Load Balancing

Load balancing is very important, particularly as you get more users and more traffic to your website. Here are the key benefits:

1. **Resource Distribution** - Distribute hundreds of thousands of requests across multiple servers so one server doesn't get bogged down
2. **Improved Performance** - Makes the experience for users much more efficient, speedier, and more pleasant to interact with
3. **High Availability** - If servers fail or need to be rebooted, NGINX ensures users always get responses from available servers
4. **Scalability** - Easy to add more servers to handle growing traffic

## Additional C# Considerations

### Adding Health Check Endpoint

For production C# applications, it's recommended to add a dedicated health check endpoint:

```csharp
var builder = WebApplication.CreateBuilder(args);
builder.Services.AddHealthChecks();

var app = builder.Build();

app.MapGet("/", () => "Hello from backend1");
app.MapHealthChecks("/health");

app.Run();
```

### NGINX Configuration with Health Checks

```nginx
upstream backend_servers {
    server 127.0.0.1:5001;
    server 127.0.0.1:5002;
    
    # Configure health checks (requires NGINX Plus or custom module)
    check interval=3000 rise=2 fall=3 timeout=1000 type=http;
    check_http_send "GET /health HTTP/1.0\r\n\r\n";
    check_http_expect_alive http_2xx;
}
```

## Summary

Using NGINX or similar services for load balancing ensures that your backend can handle high traffic volumes efficiently. By distributing traffic across multiple servers and automatically detecting server failures, you maintain high availability and optimal performance for your users.



# NGINX Load Balancing Configuration Guide

## Introduction

When you're developing a backend where you expect a lot of users to be getting data or interacting with that backend, it's important to create an environment where you can balance the load on any one server by spreading out that load, the amount of traffic going to multiple servers. Load balancing is important because it helps you distribute how much traffic each server will get.

There's software that can help you with load balancing. We're going to use an open source software package called **NGINX** (spelled N-G-I-N-X). NGINX is developed to help automatically balance the load between multiple servers or multiple applications that are running.

## Setting Up Backend Servers

### Original Node.js Example

The demonstration uses two simple Node.js servers to replicate servers that would be running applications. You can distribute the load between multiple servers either on completely separate machines or on an individual machine.

**server1.js** (listening on port 5001):
```javascript
const http = require('http');

const server = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('Hello from backend1\n');
});

const PORT = 5001;
server.listen(PORT, () => {
  console.log(`Backend1 is running on port ${PORT}`);
});
```

**server2.js** (listening on port 5002):
```javascript
const http = require('http');

const server = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('Hello from backend2\n');
});

const PORT = 5002;
server.listen(PORT, () => {
  console.log(`Backend2 is running on port ${PORT}`);
});
```

### C# ASP.NET Core Implementation

Here's how to create equivalent backend servers using C# and ASP.NET Core:

**Program.cs for Backend Server 1** (port 5001):
```csharp
var builder = WebApplication.CreateBuilder(args);

// Configure Kestrel to listen on port 5001
builder.WebHost.ConfigureKestrel(options =>
{
    options.ListenLocalhost(5001);
});

var app = builder.Build();

app.MapGet("/", () => "Hello from backend1");

Console.WriteLine("Backend1 is running on port 5001");
app.Run();
```

**Program.cs for Backend Server 2** (port 5002):
```csharp
var builder = WebApplication.CreateBuilder(args);

// Configure Kestrel to listen on port 5002
builder.WebHost.ConfigureKestrel(options =>
{
    options.ListenLocalhost(5002);
});

var app = builder.Build();

app.MapGet("/", () => "Hello from backend2");

Console.WriteLine("Backend2 is running on port 5002");
app.Run();
```

### Alternative: Single C# Project with Multiple Configurations

You can also create a single C# project and use configuration files:

**Program.cs**:
```csharp
var builder = WebApplication.CreateBuilder(args);

// Read port from configuration or command line
var port = builder.Configuration.GetValue<int>("Port", 5001);
var serverName = builder.Configuration.GetValue<string>("ServerName", "backend1");

builder.WebHost.ConfigureKestrel(options =>
{
    options.ListenLocalhost(port);
});

var app = builder.Build();

app.MapGet("/", () => $"Hello from {serverName}");

Console.WriteLine($"{serverName} is running on port {port}");
app.Run();
```

Run with command line arguments:
```bash
# Terminal 1
dotnet run --Port=5001 --ServerName=backend1

# Terminal 2
dotnet run --Port=5002 --ServerName=backend2
```

## NGINX Configuration

### Basic Configuration

The NGINX configuration defines upstream backend servers and how to distribute traffic:

**nginx.conf**:
```nginx
http {
    upstream backend_servers {
        server 127.0.0.1:5001;
        server 127.0.0.1:5002;
    }

    server {
        listen 80;
        server_name localhost;

        location / {
            proxy_pass http://backend_servers;
        }
    }
}
```

This configuration tells NGINX:
- You have two servers available on ports 5001 and 5002
- NGINX should listen on port 80 for incoming requests
- Requests to localhost should be distributed across the backend servers

### Configuration with Health Checks

You can also configure NGINX to manage server health:

```nginx
http {
    upstream backend_servers {
        server 127.0.0.1:5001 max_fails=3 fail_timeout=30s;
        server 127.0.0.1:5002 max_fails=3 fail_timeout=30s;
    }

    server {
        listen 80;
        server_name localhost;

        location / {
            proxy_pass http://backend_servers;
            proxy_next_upstream error timeout invalid_header http_500;
        }
    }
}
```

## Running the Demonstration

### Starting the Servers

**For Node.js:**
```bash
# Terminal 1
node server1.js

# Terminal 2
node server2.js
```

**For C#:**
```bash
# Terminal 1
cd BackendServer1
dotnet run

# Terminal 2
cd BackendServer2
dotnet run
```

### Testing Load Distribution

Once both servers are running, you can test the load balancing using curl:

```bash
curl http://localhost
```

**First request output:**
```
Hello from backend1
```

**Second request output:**
```
Hello from backend2
```

**Third request output:**
```
Hello from backend1
```

NGINX distributes the load using a round-robin algorithm by default, alternating between the two servers with each request.

## Health Check Demonstration

### Simulating Server Failure

To illustrate health management, stop server2. If NGINX detects that server2 is no longer available, it will automatically distribute all traffic to the only active server.

**Stop server2** (Ctrl+C in the terminal running server2)

**Run curl command again:**
```bash
curl http://localhost
```

**Every subsequent request will return:**
```
Hello from backend1
```

NGINX automatically detects that server2 is down and routes all traffic to backend1, ensuring continuous service availability.

## Benefits of Load Balancing

Load balancing is very important, particularly as you get more users and more traffic to your website. Here are the key benefits:

1. **Resource Distribution** - Distribute hundreds of thousands of requests across multiple servers so one server doesn't get bogged down
2. **Improved Performance** - Makes the experience for users much more efficient, speedier, and more pleasant to interact with
3. **High Availability** - If servers fail or need to be rebooted, NGINX ensures users always get responses from available servers
4. **Scalability** - Easy to add more servers to handle growing traffic

## Additional C# Considerations

### Adding Health Check Endpoint

For production C# applications, it's recommended to add a dedicated health check endpoint:

```csharp
var builder = WebApplication.CreateBuilder(args);
builder.Services.AddHealthChecks();

var app = builder.Build();

app.MapGet("/", () => "Hello from backend1");
app.MapHealthChecks("/health");

app.Run();
```

### NGINX Configuration with Health Checks

```nginx
upstream backend_servers {
    server 127.0.0.1:5001;
    server 127.0.0.1:5002;
    
    # Configure health checks (requires NGINX Plus or custom module)
    check interval=3000 rise=2 fall=3 timeout=1000 type=http;
    check_http_send "GET /health HTTP/1.0\r\n\r\n";
    check_http_expect_alive http_2xx;
}
```

## Installing and Deploying NGINX

### Windows Installation

1. **Download NGINX**
   - Visit [nginx.org/en/download.html](http://nginx.org/en/download.html)
   - Download the Windows version (e.g., `nginx-1.24.0.zip`)

2. **Extract NGINX**
   ```powershell
   # Extract to a location like C:\nginx
   Expand-Archive -Path nginx-1.24.0.zip -DestinationPath C:\
   ```

3. **NGINX Directory Structure**
   ```
   C:\nginx\
   â”œâ”€â”€ conf\
   â”‚   â””â”€â”€ nginx.conf          # Main configuration file
   â”œâ”€â”€ logs\
   â”‚   â”œâ”€â”€ access.log
   â”‚   â””â”€â”€ error.log
   â”œâ”€â”€ html\                   # Default web root
   â””â”€â”€ nginx.exe               # Main executable
   ```

### Linux/Ubuntu Installation

```bash
# Update package list
sudo apt update

# Install NGINX
sudo apt install nginx

# NGINX will be installed with this structure:
# /etc/nginx/nginx.conf          # Main configuration
# /etc/nginx/sites-available/    # Available site configs
# /etc/nginx/sites-enabled/      # Enabled site configs
# /var/log/nginx/                # Log files
```

### macOS Installation

```bash
# Using Homebrew
brew install nginx

# NGINX locations:
# /usr/local/etc/nginx/nginx.conf     # Main configuration
# /usr/local/var/log/nginx/           # Log files
```

## Configuring nginx.conf

### Windows Configuration Location

**File path:** `C:\nginx\conf\nginx.conf`

Replace the entire contents with:

```nginx
worker_processes  1;

events {
    worker_connections  1024;
}

http {
    include       mime.types;
    default_type  application/octet-stream;

    sendfile        on;
    keepalive_timeout  65;

    # Define upstream backend servers
    upstream backend_servers {
        server 127.0.0.1:5001;
        server 127.0.0.1:5002;
    }

    server {
        listen       80;
        server_name  localhost;

        location / {
            proxy_pass http://backend_servers;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }

        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
}
```

### Linux Configuration Location

**File path:** `/etc/nginx/nginx.conf`

Or create a custom configuration in `/etc/nginx/sites-available/load-balancer`:

```nginx
upstream backend_servers {
    server 127.0.0.1:5001;
    server 127.0.0.1:5002;
}

server {
    listen 80;
    server_name localhost;

    location / {
        proxy_pass http://backend_servers;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }
}
```

Then enable it:
```bash
sudo ln -s /etc/nginx/sites-available/load-balancer /etc/nginx/sites-enabled/
sudo nginx -t  # Test configuration
sudo systemctl reload nginx
```

### macOS Configuration Location

**File path:** `/usr/local/etc/nginx/nginx.conf`

Use the same configuration format as Windows.

## Starting and Managing NGINX

### Windows Commands

```powershell
# Navigate to NGINX directory
cd C:\nginx

# Start NGINX
start nginx

# Or run directly
.\nginx.exe

# Stop NGINX
.\nginx.exe -s stop

# Graceful stop (wait for workers to finish)
.\nginx.exe -s quit

# Reload configuration (without stopping)
.\nginx.exe -s reload

# Test configuration
.\nginx.exe -t

# Check if NGINX is running
tasklist /fi "imagename eq nginx.exe"
```

### Linux Commands

```bash
# Start NGINX
sudo systemctl start nginx

# Stop NGINX
sudo systemctl stop nginx

# Restart NGINX
sudo systemctl restart nginx

# Reload configuration (no downtime)
sudo systemctl reload nginx

# Enable NGINX to start on boot
sudo systemctl enable nginx

# Check NGINX status
sudo systemctl status nginx

# Test configuration
sudo nginx -t

# View logs
sudo tail -f /var/log/nginx/access.log
sudo tail -f /var/log/nginx/error.log
```

### macOS Commands

```bash
# Start NGINX
nginx

# Stop NGINX
nginx -s stop

# Graceful stop
nginx -s quit

# Reload configuration
nginx -s reload

# Test configuration
nginx -t

# Using Homebrew services
brew services start nginx
brew services stop nginx
brew services restart nginx
```

## Complete Step-by-Step Setup

### Step 1: Start Your Backend Servers

**For C# Servers:**
```bash
# Terminal 1
cd BackendServer1
dotnet run
# Output: Backend1 is running on port 5001

# Terminal 2
cd BackendServer2
dotnet run
# Output: Backend2 is running on port 5002
```

### Step 2: Configure NGINX

Edit the appropriate `nginx.conf` file for your OS (see locations above) and add the load balancing configuration.

### Step 3: Test NGINX Configuration

**Windows:**
```powershell
cd C:\nginx
.\nginx.exe -t
```

**Linux/macOS:**
```bash
sudo nginx -t
```

Expected output:
```
nginx: the configuration file C:\nginx\conf\nginx.conf syntax is ok
nginx: configuration file C:\nginx\conf\nginx.conf test is successful
```

### Step 4: Start NGINX

**Windows:**
```powershell
cd C:\nginx
start nginx
```

**Linux:**
```bash
sudo systemctl start nginx
```

**macOS:**
```bash
nginx
```

### Step 5: Test Load Balancing

```bash
# Make multiple requests
curl http://localhost
curl http://localhost
curl http://localhost
```

Expected rotating output:
```
Hello from backend1
Hello from backend2
Hello from backend1
```

## Troubleshooting

### Common Issues

**Port 80 Already in Use:**
```nginx
# Change the listen port in nginx.conf
server {
    listen       8080;  # Use a different port
    server_name  localhost;
}
```

Then access via: `http://localhost:8080`

**Permission Denied (Linux/macOS):**
```bash
# Use sudo for privileged ports (80, 443)
sudo nginx

# Or configure NGINX to use unprivileged ports
```

**Backend Connection Refused:**
- Ensure both backend servers are running
- Check firewall settings
- Verify correct ports in nginx.conf

### Viewing Logs

**Windows:**
```powershell
# Access logs
Get-Content C:\nginx\logs\access.log -Tail 20
# Error logs
Get-Content C:\nginx\logs\error.log -Tail 20
```

**Linux:**
```bash
sudo tail -f /var/log/nginx/access.log
sudo tail -f /var/log/nginx/error.log
```

**macOS:**
```bash
tail -f /usr/local/var/log/nginx/access.log
tail -f /usr/local/var/log/nginx/error.log
```

## Production Considerations

### Running NGINX as a Windows Service

```powershell
# Using NSSM (Non-Sucking Service Manager)
# Download from nssm.cc

nssm install nginx "C:\nginx\nginx.exe"
nssm start nginx
```

### Firewall Configuration

**Windows:**
```powershell
netsh advfirewall firewall add rule name="NGINX HTTP" dir=in action=allow protocol=TCP localport=80
```

**Linux (UFW):**
```bash
sudo ufw allow 'Nginx HTTP'
sudo ufw allow 'Nginx HTTPS'
```

### Advanced nginx.conf with Health Checks

```nginx
user  nginx;
worker_processes  auto;

events {
    worker_connections  1024;
}

http {
    upstream backend_servers {
        server 127.0.0.1:5001 max_fails=3 fail_timeout=30s;
        server 127.0.0.1:5002 max_fails=3 fail_timeout=30s;
        
        # Load balancing method (default is round-robin)
        # least_conn;  # Send to server with least connections
        # ip_hash;     # Same client always goes to same server
    }

    server {
        listen 80;
        server_name localhost;

        location / {
            proxy_pass http://backend_servers;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            
            # Timeout settings
            proxy_connect_timeout 60s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
            
            # Retry on next server if current fails
            proxy_next_upstream error timeout invalid_header http_500 http_502 http_503;
        }

        # Health check endpoint (optional)
        location /nginx-health {
            access_log off;
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }
    }
}

```

## Summary

Using NGINX or similar services for load balancing ensures that your backend can handle high traffic volumes efficiently. By distributing traffic across multiple servers and automatically detecting server failures, you maintain high availability and optimal performance for your users.

**Key Points:**
- Place `nginx.conf` in the appropriate directory for your OS
- Always test configuration before reloading: `nginx -t`
- Use reload instead of restart for zero-downtime configuration updates
- Monitor logs to troubleshoot issues
- Consider using systemd (Linux) or Windows services for production deployments






Great question, Andreas ðŸ‘Œ. Letâ€™s break this `nginx.conf` down line by line so you understand exactly what each directive does and why itâ€™s there.  

---

## ðŸ”¹ Upstream Block
```nginx
upstream backend_servers {
    server 127.0.0.1:5001 max_fails=3 fail_timeout=30s;
    server 127.0.0.1:5002 max_fails=3 fail_timeout=30s;
    
    # Load balancing method (default is round-robin)
    # least_conn;  # Send to server with least connections
    # ip_hash;     # Same client always goes to same server
}
```

- **`upstream backend_servers { ... }`**  
  Defines a group of backend servers (a pool) that Nginx can proxy requests to. This is the load-balancing configuration.

- **`server 127.0.0.1:5001 max_fails=3 fail_timeout=30s;`**  
  - Points to a backend server running locally on port 5001.  
  - `max_fails=3`: If the server fails 3 times in a row, itâ€™s marked as unavailable.  
  - `fail_timeout=30s`: After 30 seconds, Nginx will retry the server.

- **Load balancing methods (commented out):**  
  - **Default (round-robin):** Requests are distributed evenly across servers.  
  - **`least_conn;`**: Routes traffic to the server with the fewest active connections.  
  - **`ip_hash;`**: Ensures the same client IP always goes to the same backend server (useful for session persistence).

---

## ðŸ”¹ Server Block
```nginx
server {
    listen 80;
    server_name localhost;
```

- **`listen 80;`**  
  Nginx listens on port 80 (HTTP).  

- **`server_name localhost;`**  
  Matches requests for `localhost`.  

---

## ðŸ”¹ Location `/`
```nginx
    location / {
        proxy_pass http://backend_servers;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        
        # Timeout settings
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
        
        # Retry on next server if current fails
        proxy_next_upstream error timeout invalid_header http_500 http_502 http_503;
    }
```

- **`proxy_pass http://backend_servers;`**  
  Forwards requests to the upstream group defined earlier.  

- **Headers passed to backend:**  
  - `Host $host`: Preserves the original host header.  
  - `X-Real-IP $remote_addr`: Passes the clientâ€™s real IP.  
  - `X-Forwarded-For $proxy_add_x_forwarded_for`: Adds client IP to the chain of forwarded addresses (important for logging and security).  

- **Timeouts:**  
  - `proxy_connect_timeout 60s`: Max time to establish a connection to backend.  
  - `proxy_send_timeout 60s`: Max time to send request to backend.  
  - `proxy_read_timeout 60s`: Max time to wait for a response.  

- **`proxy_next_upstream ...`**  
  If the current backend fails (due to error, timeout, invalid header, or specific HTTP errors like 500/502/503), Nginx retries the next server in the upstream pool.

---

## ðŸ”¹ Health Check Endpoint
```nginx
    location /nginx-health {
        access_log off;
        return 200 "healthy\n";
        add_header Content-Type text/plain;
    }
}
```

- **Purpose:** Provides a simple endpoint (`/nginx-health`) that returns `200 OK` with the text `"healthy"`.  
- **`access_log off;`**: Prevents logging requests to this endpoint.  
- **`return 200 "healthy\n";`**: Always responds with HTTP 200 and a plain text message.  
- **`add_header Content-Type text/plain;`**: Ensures the response is plain text.  

This is often used by monitoring systems (like Kubernetes, Docker health checks, or load balancers) to verify Nginx is alive.

---

## âœ… Summary
- **Upstream block:** Defines backend servers and load-balancing strategy.  
- **Server block:** Listens on port 80 for `localhost`.  
- **Location `/`:** Proxies requests to backend servers with headers, timeouts, and retry logic.  
- **Location `/nginx-health`:** Simple health check endpoint for monitoring.  

---

Would you like me to also **diagram the request flow** (client â†’ Nginx â†’ backend servers) so you can visualize how traffic moves through this config?