# NGINX Load Balancing Lab Guide

## Introduction

When you're working with multiple servers and you want to make sure that your users always serve the pages that you need, using a load balancing service can be super helpful. In this lab, we're going to implement load balancing in a simulated environment using NGINX or AWS Elastic Load Balancing. In this particular context, we're going to use the open source service NGINX.

## Lab Overview

We will:
1. Configure NGINX as a load balancer
2. Enable health checks in NGINX
3. Test failover scenarios

### Architecture

- **Backend Server 1** - Running on port 5001
- **Backend Server 2** - Running on port 5002
- **NGINX** - Load balancer distributing requests between both servers
- **Clients** - Connect to localhost (port 80) through NGINX

## Step 1: Configure NGINX as a Load Balancer

### Backend Server Implementation

#### Node.js Implementation (Original)

**backend1.js** (Port 5001):
```javascript
const http = require('http');

const server = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('Hello from Backend 1\n');
});

const PORT = 5001;
server.listen(PORT, () => {
  console.log(`Backend 1 is running on port ${PORT}`);
});
```

**backend2.js** (Port 5002):
```javascript
const http = require('http');

const server = http.createCreate((req, res) => {
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('Hello from Backend 2\n');
});

const PORT = 5002;
server.listen(PORT, () => {
  console.log(`Backend 2 is running on port ${PORT}`);
});
```

#### C# ASP.NET Core Implementation

**Backend1/Program.cs** (Port 5001):
```csharp
var builder = WebApplication.CreateBuilder(args);

builder.WebHost.ConfigureKestrel(options =>
{
    options.ListenLocalhost(5001);
});

var app = builder.Build();

app.MapGet("/", () => Results.Content(
    "Hello from Backend 1", 
    "text/plain"
));

Console.WriteLine("Backend 1 is running on port 5001");
app.Run();
```

**Backend2/Program.cs** (Port 5002):
```csharp
var builder = WebApplication.CreateBuilder(args);

builder.WebHost.ConfigureKestrel(options =>
{
    options.ListenLocalhost(5002);
});

var app = builder.Build();

app.MapGet("/", () => Results.Content(
    "Hello from Backend 2", 
    "text/plain"
));

Console.WriteLine("Backend 2 is running on port 5002");
app.Run();
```

#### Alternative: Configurable C# Server

**Program.cs** (Single project for both servers):
```csharp
var builder = WebApplication.CreateBuilder(args);

// Read configuration from command line or environment
var port = args.Length > 0 ? int.Parse(args[0]) : 5001;
var serverName = args.Length > 1 ? args[1] : "Backend 1";

builder.WebHost.ConfigureKestrel(options =>
{
    options.ListenLocalhost(port);
});

var app = builder.Build();

app.MapGet("/", () => Results.Content(
    $"Hello from {serverName}", 
    "text/plain"
));

// Add health check endpoint
app.MapGet("/health", () => Results.Ok(new 
{ 
    status = "healthy", 
    server = serverName,
    timestamp = DateTime.UtcNow 
}));

Console.WriteLine($"{serverName} is running on port {port}");
app.Run();
```

**Running multiple instances:**
```bash
# Terminal 1
dotnet run 5001 "Backend 1"

# Terminal 2
dotnet run 5002 "Backend 2"
```

### Modify NGINX Configuration File

**File Location:** `C:\nginx\conf\nginx.conf` (Windows) or `/etc/nginx/nginx.conf` (Linux/macOS)

Find the `http` block and add the upstream and server configuration:

```nginx
http {
    include       mime.types;
    default_type  application/octet-stream;

    sendfile        on;
    keepalive_timeout  65;

    # Define upstream backend servers
    upstream backend_servers {
        server 127.0.0.1:5001;
        server 127.0.0.1:5002;
    }

    # Configure the load balancer
    server {
        listen       80;
        server_name  localhost;

        location / {
            proxy_pass http://backend_servers;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }

        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
}
```

### Configuration Explanation

- **upstream backend_servers**: Defines where NGINX should look for servers to distribute the load
- **server 127.0.0.1:5001**: The IP address and port for Backend 1
- **server 127.0.0.1:5002**: The IP address and port for Backend 2
- **listen 80**: NGINX listens on port 80
- **proxy_pass**: Routes requests to one of the backend servers

### Start the Backend Servers

**Node.js:**
```bash
# Terminal 1
node backend1.js
# Output: Backend 1 is running on port 5001

# Terminal 2
node backend2.js
# Output: Backend 2 is running on port 5002
```

**C#:**
```bash
# Terminal 1
cd Backend1
dotnet run
# Output: Backend 1 is running on port 5001

# Terminal 2
cd Backend2
dotnet run
# Output: Backend 2 is running on port 5002
```

### Test Load Balancing

1. **Open Browser** - Navigate to `http://localhost`
2. **First Request** - You'll see: "Hello from Backend 1"
3. **Refresh Page** - You'll see: "Hello from Backend 2"
4. **Refresh Again** - Back to: "Hello from Backend 1"

NGINX uses round-robin load balancing by default, alternating between servers.

### Test Failover Scenario

1. **Stop Backend 2** - Press `Ctrl+C` in Terminal 2
2. **Refresh Browser** - After a brief delay (while NGINX detects the failure), all requests go to Backend 1
3. **Restart Backend 2** - NGINX automatically detects it's back online and resumes load balancing

## Step 2: Enable Health Checks in NGINX

### Update NGINX Configuration with Health Checks

Modify the `upstream backend_servers` block in `nginx.conf`:

```nginx
upstream backend_servers {
    server 127.0.0.1:5001 max_fails=3 fail_timeout=10s;
    server 127.0.0.1:5002 max_fails=3 fail_timeout=10s;
}

server {
    listen       80;
    server_name  localhost;

    location / {
        proxy_pass http://backend_servers;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        
        # Connection and timeout settings
        proxy_connect_timeout 5s;
        proxy_send_timeout 10s;
        proxy_read_timeout 10s;
        
        # Retry on next server if current fails
        proxy_next_upstream error timeout invalid_header http_500 http_502 http_503;
    }
}
```

### Health Check Parameters Explained

- **max_fails=3**: Server is marked as unavailable after 3 failed attempts
- **fail_timeout=10s**: Server remains unavailable for 10 seconds before NGINX tries again
- **proxy_connect_timeout**: Maximum time to establish connection with backend
- **proxy_send_timeout**: Maximum time for sending request to backend
- **proxy_read_timeout**: Maximum time for reading response from backend
- **proxy_next_upstream**: Conditions that trigger trying the next server

### Reload NGINX Configuration

**Windows:**
```powershell
cd C:\nginx
.\nginx.exe -s reload
```

**Linux/macOS:**
```bash
sudo nginx -s reload
```

### Enhanced C# Backend with Logging

To better demonstrate health checks and failures, add logging to your C# backends:

```csharp
var builder = WebApplication.CreateBuilder(args);

var port = args.Length > 0 ? int.Parse(args[0]) : 5001;
var serverName = args.Length > 1 ? args[1] : "Backend 1";

builder.WebHost.ConfigureKestrel(options =>
{
    options.ListenLocalhost(port);
});

// Add logging
builder.Logging.ClearProviders();
builder.Logging.AddConsole();

var app = builder.Build();
var logger = app.Logger;

app.Use(async (context, next) =>
{
    logger.LogInformation(
        "[{Time}] Request from {IP} to {Path}",
        DateTime.Now.ToString("HH:mm:ss"),
        context.Connection.RemoteIpAddress,
        context.Request.Path
    );
    await next();
});

app.MapGet("/", () => 
{
    return Results.Content($"Hello from {serverName}", "text/plain");
});

app.MapGet("/health", () => 
{
    return Results.Ok(new 
    { 
        status = "healthy", 
        server = serverName,
        timestamp = DateTime.UtcNow,
        uptime = Environment.TickCount64 / 1000.0
    });
});

// Simulate slow response for testing timeouts
app.MapGet("/slow", async () => 
{
    await Task.Delay(15000); // 15 second delay
    return Results.Content($"Slow response from {serverName}", "text/plain");
});

// Simulate server error for testing failover
app.MapGet("/error", () => 
{
    return Results.StatusCode(500);
});

Console.WriteLine($"{serverName} is running on port {port}");
app.Run();
```

### Testing Health Checks

1. **Test Timeout** - Visit `http://localhost/slow`
   - NGINX will timeout after 10 seconds and try the next server

2. **Test Error Handling** - Visit `http://localhost/error`
   - NGINX receives 500 error and tries the next server

3. **Test Multiple Failures** - Stop one server and make 3+ requests quickly
   - NGINX marks the server as down and routes all traffic to healthy server

## Database Integration Example with T-SQL

### C# Backend with SQL Server Connection

For a more realistic scenario, here's a backend that queries a database:

```csharp
using Microsoft.Data.SqlClient;
using System.Data;

var builder = WebApplication.CreateBuilder(args);

var port = args.Length > 0 ? int.Parse(args[0]) : 5001;
var serverName = args.Length > 1 ? args[1] : "Backend 1";

// Connection string (same database for both servers)
var connectionString = builder.Configuration.GetConnectionString("DefaultConnection") 
    ?? "Server=localhost;Database=LoadBalancerDemo;Integrated Security=true;TrustServerCertificate=true;";

builder.WebHost.ConfigureKestrel(options =>
{
    options.ListenLocalhost(port);
});

var app = builder.Build();

app.MapGet("/", () => Results.Content($"Hello from {serverName}", "text/plain"));

// Endpoint to get server statistics from database
app.MapGet("/api/stats", async () =>
{
    try
    {
        using var connection = new SqlConnection(connectionString);
        await connection.OpenAsync();

        // Log this request
        using (var cmd = new SqlCommand(
            "INSERT INTO ServerRequests (ServerName, RequestTime) VALUES (@ServerName, @RequestTime)", 
            connection))
        {
            cmd.Parameters.AddWithValue("@ServerName", serverName);
            cmd.Parameters.AddWithValue("@RequestTime", DateTime.UtcNow);
            await cmd.ExecuteNonQueryAsync();
        }

        // Get statistics
        using (var cmd = new SqlCommand(
            @"SELECT 
                ServerName, 
                COUNT(*) as TotalRequests,
                MAX(RequestTime) as LastRequest
              FROM ServerRequests 
              GROUP BY ServerName", 
            connection))
        {
            var stats = new List<object>();
            using var reader = await cmd.ExecuteReaderAsync();
            
            while (await reader.ReadAsync())
            {
                stats.Add(new
                {
                    server = reader.GetString(0),
                    totalRequests = reader.GetInt32(1),
                    lastRequest = reader.GetDateTime(2)
                });
            }
            
            return Results.Ok(new { currentServer = serverName, statistics = stats });
        }
    }
    catch (Exception ex)
    {
        return Results.Problem($"Database error: {ex.Message}");
    }
});

app.MapGet("/health", async () =>
{
    try
    {
        using var connection = new SqlConnection(connectionString);
        await connection.OpenAsync();
        
        return Results.Ok(new 
        { 
            status = "healthy", 
            server = serverName,
            database = "connected",
            timestamp = DateTime.UtcNow 
        });
    }
    catch
    {
        return Results.StatusCode(503); // Service Unavailable
    }
});

Console.WriteLine($"{serverName} is running on port {port}");
app.Run();
```

### T-SQL Database Setup

**Create Database and Table:**
```sql
-- Create database
CREATE DATABASE LoadBalancerDemo;
GO

USE LoadBalancerDemo;
GO

-- Create table to track requests
CREATE TABLE ServerRequests (
    Id INT IDENTITY(1,1) PRIMARY KEY,
    ServerName NVARCHAR(50) NOT NULL,
    RequestTime DATETIME2 NOT NULL,
    CONSTRAINT DF_ServerRequests_RequestTime DEFAULT GETUTCDATE() FOR RequestTime
);
GO

-- Create index for better query performance
CREATE INDEX IX_ServerRequests_ServerName 
ON ServerRequests(ServerName);
GO

CREATE INDEX IX_ServerRequests_RequestTime 
ON ServerRequests(RequestTime DESC);
GO
```

**Query to Monitor Load Distribution:**
```sql
-- View request distribution across servers
SELECT 
    ServerName,
    COUNT(*) as TotalRequests,
    MIN(RequestTime) as FirstRequest,
    MAX(RequestTime) as LastRequest,
    CAST(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM ServerRequests) AS DECIMAL(5,2)) as PercentageOfTotal
FROM ServerRequests
GROUP BY ServerName
ORDER BY TotalRequests DESC;
```

**Query to View Recent Activity:**
```sql
-- View last 20 requests
SELECT TOP 20
    Id,
    ServerName,
    RequestTime,
    DATEDIFF(SECOND, LAG(RequestTime) OVER (ORDER BY RequestTime), RequestTime) as SecondsSincePrevious
FROM ServerRequests
ORDER BY RequestTime DESC;
```

**Stored Procedure for Health Check:**
```sql
CREATE PROCEDURE sp_CheckServerHealth
    @ServerName NVARCHAR(50)
AS
BEGIN
    SET NOCOUNT ON;
    
    -- Get server statistics
    SELECT 
        @ServerName as ServerName,
        COUNT(*) as TotalRequests,
        MAX(RequestTime) as LastRequestTime,
        DATEDIFF(MINUTE, MAX(RequestTime), GETUTCDATE()) as MinutesSinceLastRequest,
        CASE 
            WHEN DATEDIFF(MINUTE, MAX(RequestTime), GETUTCDATE()) <= 5 THEN 'Healthy'
            WHEN DATEDIFF(MINUTE, MAX(RequestTime), GETUTCDATE()) <= 15 THEN 'Warning'
            ELSE 'Unhealthy'
        END as HealthStatus
    FROM ServerRequests
    WHERE ServerName = @ServerName;
END;
GO

-- Execute health check
EXEC sp_CheckServerHealth @ServerName = 'Backend 1';
```

## Advanced NGINX Configuration

### Load Balancing Methods

```nginx
upstream backend_servers {
    # Round-robin (default) - distributes requests evenly
    # No directive needed
    
    # Least connections - sends to server with fewest active connections
    # least_conn;
    
    # IP hash - same client always goes to same server (sticky sessions)
    # ip_hash;
    
    # Weighted round-robin - server with higher weight gets more requests
    # server 127.0.0.1:5001 weight=3;
    # server 127.0.0.1:5002 weight=1;
    
    server 127.0.0.1:5001 max_fails=3 fail_timeout=10s;
    server 127.0.0.1:5002 max_fails=3 fail_timeout=10s;
}
```

### Complete Production-Ready Configuration

```nginx
http {
    include       mime.types;
    default_type  application/octet-stream;

    # Logging
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for" '
                    'upstream: $upstream_addr';

    access_log  logs/access.log  main;
    error_log   logs/error.log   warn;

    sendfile        on;
    keepalive_timeout  65;

    # Backend servers
    upstream backend_servers {
        least_conn;  # Use least connections algorithm
        
        server 127.0.0.1:5001 max_fails=3 fail_timeout=10s weight=1;
        server 127.0.0.1:5002 max_fails=3 fail_timeout=10s weight=1;
        
        # Keepalive connections to backends
        keepalive 32;
    }

    server {
        listen       80;
        server_name  localhost;

        # Main application
        location / {
            proxy_pass http://backend_servers;
            
            # Headers
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # Timeouts
            proxy_connect_timeout 5s;
            proxy_send_timeout 10s;
            proxy_read_timeout 10s;
            
            # Retry logic
            proxy_next_upstream error timeout invalid_header http_500 http_502 http_503;
            proxy_next_upstream_tries 2;
            
            # Buffering
            proxy_buffering on;
            proxy_buffer_size 4k;
            proxy_buffers 8 4k;
        }

        # Health check endpoint (doesn't hit backends)
        location /nginx-status {
            stub_status on;
            access_log off;
            allow 127.0.0.1;
            deny all;
        }

        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
}
```

## Testing and Validation

### PowerShell Script to Test Load Balancing

```powershell
# Test load balancing distribution
Write-Host "Testing load balancing..." -ForegroundColor Green

for ($i = 1; $i -le 10; $i++) {
    $response = Invoke-WebRequest -Uri "http://localhost" -UseBasicParsing
    Write-Host "Request $i : $($response.Content.Trim())"
    Start-Sleep -Milliseconds 500
}

Write-Host "`nTesting health check..." -ForegroundColor Green
$health = Invoke-WebRequest -Uri "http://localhost/health" -UseBasicParsing
Write-Host $health.Content
```

### Bash Script to Test Load Balancing

```bash
#!/bin/bash

echo "Testing load balancing..."

for i in {1..10}
do
    response=$(curl -s http://localhost)
    echo "Request $i: $response"
    sleep 0.5
done

echo -e "\nTesting health check..."
curl -s http://localhost/health | jq
```

## Summary

This lab demonstrates how to use NGINX as a load balancer with health checks. Key takeaways:

1. **Load Distribution** - NGINX automatically distributes traffic across multiple backend servers
2. **Health Monitoring** - Failed servers are automatically taken out of rotation
3. **Zero Downtime** - Maintenance can be performed on individual servers without affecting users
4. **Scalability** - Easy to add more backend servers as needed
5. **Database Integration** - All servers can connect to the same database for shared state

**Benefits:**
- More responsive environment for users
- Automatic failover when servers go down
- Even distribution of load across servers
- Simple configuration and management

With tools like NGINX, you can ensure your users always have a responsive and available website, even with multiple servers and complex infrastructure.